{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. tokenizer\n",
    "\n",
    "- 문장을 의미있는 단위로 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install konlpy \n",
    "# !pip install kss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Using cached kss-1.3.1.tar.gz (6.1 kB)\n",
      "Building wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for kss\n",
      "Failed to build kss\n",
      "Installing collected packages: kss\n",
      "    Running setup.py install for kss: started\n",
      "    Running setup.py install for kss: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\gjai_kmj\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\gjai_kmj\\AppData\\Local\\Temp\\pip-wheel-hcq2d4_p'\n",
      "       cwd: C:\\Users\\gjai_kmj\\AppData\\Local\\Temp\\pip-install-6jthomgg\\kss\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'kss' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for kss\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\gjai_kmj\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\gjai_kmj\\AppData\\Local\\Temp\\pip-record-tb5wwkiu\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\gjai_kmj\\anaconda3\\Include\\kss'\n",
      "         cwd: C:\\Users\\gjai_kmj\\AppData\\Local\\Temp\\pip-install-6jthomgg\\kss\\\n",
      "    Complete output (5 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'kss' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\gjai_kmj\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\gjai_kmj\\\\AppData\\\\Local\\\\Temp\\\\pip-install-6jthomgg\\\\kss\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\gjai_kmj\\AppData\\Local\\Temp\\pip-record-tb5wwkiu\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\gjai_kmj\\anaconda3\\Include\\kss' Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install kss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gjai_kmj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'only', 'one', 'call', 'away', '.', 'I', \"'ll\", 'be', 'there', 'to', 'save', 'the', 'day', '.', 'Superman', 'got', 'nothing', 'on', 'me', ',', 'I', \"'m\", 'only', 'one', 'call', 'away', '.', 'And', 'when', 'you', \"'re\", 'weak', ',', 'I', \"'ll\", 'be', 'strong', ',', 'I', \"'m\", 'gon', 'na', 'keep', 'holding', 'on', '.', 'Now', 'do', \"n't\", 'you', 'worry', ',', 'it', 'wo', \"n't\", 'be', 'long', ',', 'darling', '.', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # tokenizer\n",
    "\n",
    "text = \"\"\"\n",
    "I'm only one call away.\n",
    "I'll be there to save the day.\n",
    "Superman got nothing on me, I'm only one call away.\n",
    "And when you're weak, I'll be strong, I'm gonna keep holding on.\n",
    "Now don't you worry, it won't be long, darling.\n",
    "And when you feel like hope is gone just run into my arms.\n",
    "\"\"\"\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'm', 'only', 'one', 'call', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'to', 'save', 'the', 'day', '.', 'Superman', 'got', 'nothing', 'on', 'me', ',', 'I', \"'\", 'm', 'only', 'one', 'call', 'away', '.', 'And', 'when', 'you', \"'\", 're', 'weak', ',', 'I', \"'\", 'll', 'be', 'strong', ',', 'I', \"'\", 'm', 'gonna', 'keep', 'holding', 'on', '.', 'Now', 'don', \"'\", 't', 'you', 'worry', ',', 'it', 'won', \"'\", 't', 'be', 'long', ',', 'darling', '.', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer # tokenizer\n",
    "\n",
    "print(WordPunctTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'only', 'one', 'call', 'away.', 'I', \"'ll\", 'be', 'there', 'to', 'save', 'the', 'day.', 'Superman', 'got', 'nothing', 'on', 'me', ',', 'I', \"'m\", 'only', 'one', 'call', 'away.', 'And', 'when', 'you', \"'re\", 'weak', ',', 'I', \"'ll\", 'be', 'strong', ',', 'I', \"'m\", 'gon', 'na', 'keep', 'holding', 'on.', 'Now', 'do', \"n't\", 'you', 'worry', ',', 'it', 'wo', \"n't\", 'be', 'long', ',', 'darling.', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms', '.']\n"
     ]
    }
   ],
   "source": [
    "# 가장 많이 사용하는 tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "# word_tokenize와 비슷하지만 하이픈을 포함해서 처리하는 특이점이 있다.\n",
    "\n",
    "print(TreebankWordTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'm', 'only', 'one', 'call', 'away', 'I', 'll', 'be', 'there', 'to', 'save', 'the', 'day', 'Superman', 'got', 'nothing', 'on', 'me', 'I', 'm', 'only', 'one', 'call', 'away', 'And', 'when', 'you', 're', 'weak', 'I', 'll', 'be', 'strong', 'I', 'm', 'gonna', 'keep', 'holding', 'on', 'Now', 'don', 't', 'you', 'worry', 'it', 'won', 't', 'be', 'long', 'darling', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "# 정규표현식으로 원하는 형태만 추출하는 tokenizer\n",
    "# 원하는 형태가 없는 부분을 나눔\n",
    "\n",
    "print(RegexpTokenizer(\"\\w+\").tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nI'm only one call away.\", \"I'll be there to save the day.\", \"Superman got nothing on me, I'm only one call away.\", \"And when you're weak, I'll be strong, I'm gonna keep holding on.\", \"Now don't you worry, it won't be long, darling.\", 'And when you feel like hope is gone just run into my arms.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize # 문장 tokenizer\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '?', '저', '는', '강민지', '라고', '합니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한글\n",
    "from konlpy.tag import Okt, Kkma # tokenizer\n",
    "\n",
    "ktext = '안녕하세요? 저는 강민지라고 합니다.'\n",
    "print(Okt().morphs(ktext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kss # 문장 tokenizer\n",
    "\n",
    "# print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "onli\n",
      "one\n",
      "call\n",
      "away\n",
      ".\n",
      "I\n",
      "'ll\n",
      "be\n",
      "there\n",
      "to\n",
      "save\n",
      "the\n",
      "day\n",
      ".\n",
      "superman\n",
      "got\n",
      "noth\n",
      "on\n",
      "me\n",
      ",\n",
      "I\n",
      "'m\n",
      "onli\n",
      "one\n",
      "call\n",
      "away\n",
      ".\n",
      "and\n",
      "when\n",
      "you\n",
      "'re\n",
      "weak\n",
      ",\n",
      "I\n",
      "'ll\n",
      "be\n",
      "strong\n",
      ",\n",
      "I\n",
      "'m\n",
      "gon\n",
      "na\n",
      "keep\n",
      "hold\n",
      "on\n",
      ".\n",
      "now\n",
      "do\n",
      "n't\n",
      "you\n",
      "worri\n",
      ",\n",
      "it\n",
      "wo\n",
      "n't\n",
      "be\n",
      "long\n",
      ",\n",
      "darl\n",
      ".\n",
      "and\n",
      "when\n",
      "you\n",
      "feel\n",
      "like\n",
      "hope\n",
      "is\n",
      "gone\n",
      "just\n",
      "run\n",
      "into\n",
      "my\n",
      "arm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "for word in words:\n",
    "    print(PorterStemmer().stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gjai_kmj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거한 단어 모음: [\"'m\", 'one', 'call', 'away', '.', \"'ll\", 'there', 'save', 'day', '.', 'Superman', 'got', 'nothing', 'me', ',', \"'m\", 'one', 'call', 'away', '.', 'when', \"'re\", 'weak', ',', \"'ll\", 'strong', ',', \"'m\", 'gon', 'na', 'keep', 'holding', '.', 'do', \"n't\", 'worry', ',', 'wo', \"n't\", 'long', ',', 'darling', '.', 'when', 'feel', 'like', 'hope', 'gone', 'run', 'my', 'arms', '.']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "전체 단어 모음: ['I', \"'m\", 'only', 'one', 'call', 'away', '.', 'I', \"'ll\", 'be', 'there', 'to', 'save', 'the', 'day', '.', 'Superman', 'got', 'nothing', 'on', 'me', ',', 'I', \"'m\", 'only', 'one', 'call', 'away', '.', 'And', 'when', 'you', \"'re\", 'weak', ',', 'I', \"'ll\", 'be', 'strong', ',', 'I', \"'m\", 'gon', 'na', 'keep', 'holding', 'on', '.', 'Now', 'do', \"n't\", 'you', 'worry', ',', 'it', 'wo', \"n't\", 'be', 'long', ',', 'darling', '.', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms', '.']\n"
     ]
    }
   ],
   "source": [
    "# 실습 \n",
    "# words에서 stopwords를 제거하는 알고리즘 짜기\n",
    "\n",
    "words_ex = words.copy()\n",
    "\n",
    "for word in words_ex:\n",
    "    if word.lower() in sw:\n",
    "        words_ex.remove(word)\n",
    "        \n",
    "print(\"불용어 제거한 단어 모음:\", words_ex)\n",
    "print('\\n'+'-'*80)\n",
    "print(\"전체 단어 모음:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ stopword를 제거하는 것이 항상 좋은 것만은 아니다.\n",
    "\n",
    "강사님이 기사 헤드라인 분석했을 때 stopwords를 제거했더니 정확도가 매우 떨어졌음.\n",
    "간단히 제거만할 것이 아니라, 직접 판단해서 필요없는 stopword를 선택해서 제거하는 것이 좋은 방법일 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.': 6, ',': 5, \"'m\": 3, 'one': 2, 'call': 2, 'away': 2, \"'ll\": 2, 'when': 2, \"n't\": 2, 'there': 1, 'save': 1, 'day': 1, 'Superman': 1, 'got': 1, 'nothing': 1, 'me': 1, \"'re\": 1, 'weak': 1, 'strong': 1, 'gon': 1, 'na': 1, 'keep': 1, 'holding': 1, 'do': 1, 'worry': 1, 'wo': 1, 'long': 1, 'darling': 1, 'feel': 1, 'like': 1, 'hope': 1, 'gone': 1, 'run': 1, 'my': 1, 'arms': 1})\n"
     ]
    }
   ],
   "source": [
    "# 단어를 중요도 순서로 나열하기 \n",
    "# Bag Of Words - 문장에 단어가 몇 개 나왔는지 \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "count_list = Counter(words_ex)\n",
    "print(count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.`, `,`가 중요한 단어로 나타나므로 불용어 처리를 다시 해주겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'only', 'one', 'call', 'away', 'I', \"'ll\", 'be', 'there', 'to', 'save', 'the', 'day', 'Superman', 'got', 'nothing', 'on', 'me', 'I', \"'m\", 'only', 'one', 'call', 'away', 'And', 'when', 'you', \"'re\", 'weak', 'I', \"'ll\", 'be', 'strong', 'I', \"'m\", 'gon', 'na', 'keep', 'holding', 'on', 'Now', 'do', \"n't\", 'you', 'worry', 'it', 'wo', \"n't\", 'be', 'long', 'darling', 'And', 'when', 'you', 'feel', 'like', 'hope', 'is', 'gone', 'just', 'run', 'into', 'my', 'arms']\n"
     ]
    }
   ],
   "source": [
    "sw = ['.', ',']\n",
    "\n",
    "sw_removed = []\n",
    "for i in words:\n",
    "    if i.lower() not in sw:\n",
    "        sw_removed.append(i)\n",
    "        \n",
    "print(sw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I': 5, \"'m\": 3, 'be': 3, 'you': 3, 'only': 2, 'one': 2, 'call': 2, 'away': 2, \"'ll\": 2, 'on': 2, 'And': 2, 'when': 2, \"n't\": 2, 'there': 1, 'to': 1, 'save': 1, 'the': 1, 'day': 1, 'Superman': 1, 'got': 1, 'nothing': 1, 'me': 1, \"'re\": 1, 'weak': 1, 'strong': 1, 'gon': 1, 'na': 1, 'keep': 1, 'holding': 1, 'Now': 1, 'do': 1, 'worry': 1, 'it': 1, 'wo': 1, 'long': 1, 'darling': 1, 'feel': 1, 'like': 1, 'hope': 1, 'is': 1, 'gone': 1, 'just': 1, 'run': 1, 'into': 1, 'my': 1, 'arms': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_list = Counter(sw_removed)\n",
    "print(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 5), (\"'m\", 3), ('be', 3), ('you', 3), ('only', 2), ('one', 2), ('call', 2), ('away', 2), (\"'ll\", 2), ('on', 2)]\n"
     ]
    }
   ],
   "source": [
    "# top10 words\n",
    "common_cl = count_list.most_common(10)  # Counter에 들어있는 함수\n",
    "print(common_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, \"'m\": 1, 'be': 2, 'you': 3, 'only': 4, 'one': 5, 'call': 6, 'away': 7, \"'ll\": 8, 'on': 9}\n"
     ]
    }
   ],
   "source": [
    "# 중요도 순서로 단어에 index를 부여\n",
    "\n",
    "common_dict = {}\n",
    "\n",
    "for i, word in enumerate(common_cl):\n",
    "    common_dict[word[0]] = i\n",
    "\n",
    "print(common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
