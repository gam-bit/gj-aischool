{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 네이버 헤드뉴스 찾기\n",
    "![네이버 헤드뉴스](../image/naverheadnews.PNG)\n",
    "\n",
    "**Exercise:** 네이버 헤드뉴스의 제목이 list에 담기도록 크롤링 하기\n",
    "![결과1](../image/result_naverheadnews.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20일부터 수도권 박물관·미술관·도서관 공공시설 문 다시 연다', '정총리 \"8월 17일 임시공휴일 지정 고려해야\"', '靑 \"그린벨트 해제, 아직 결론 못내…종합적 고민해야\"', '추미애 연이틀 \\'부동산 정책\\' 메시지…\"가격내리기 실패는 돈탓\"', '수도권·강원·충남에 밤사이 많은 비…호우 대처 중대본 가동', '편지 언급에 \"한 건 걸리면 되지\"…한동훈-기자 녹취록 공개', '경찰, 박원순 사망 관련 임순영 젠더특보 주중 소환 예정', '\"파주 아파트 수돗물서도 유충 발견\" 신고', '이인영 \"대북전단 반드시 중단…인도물자 물물교환 방식 병행\"', \"'노영민 부동산 브리핑 정정' 강민석, 사표냈다 반려\"]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    div = soup.find('div', class_='list_issue')\n",
    "    \n",
    "    for i in div.find_all('a'):\n",
    "        result.append(i.get_text()) # get_text는 태그 안의 내용만 추출\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'http://www.naver.com'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(crawling(soup))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 연합뉴스 속보 기사들의 제목 추출하기\n",
    "\n",
    "**Exercise:** 네이버 연합뉴스의 볼드체로 된 속보 기사 제목을 추출하기\n",
    "![연합 속보](../image/result_yonhap.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '20일부터 수도권 박물관·미술관·도서관 공공시설 문 다시 연다', '정부, 교회 \\'소모임 금지\\' 해제 검토…\"최근 감염 거의 없어\"', \"서울 관악구 사무실 집단감염, 광주·제주까지 확산 '비상'\", '중대본 \"코로나19 모든 관리지표 개선…긴장 늦출 시기는 아냐\"', '호흡기전담클리닉 내년까지 1천개 설치…보건소당 1개 이상 확보', '정총리 \"8월 17일 임시공휴일 지정 고려해야\"', '靑 \"그린벨트 해제, 아직 결론 못내…종합적 고민해야\"', '추미애 연이틀 \\'부동산 정책\\' 메시지…\"가격내리기 실패는 돈탓\"', '수도권·강원·충남에 밤사이 많은 비…호우 대처 중대본 가동', '편지 언급에 \"한 건 걸리면 되지\"…한동훈-기자 녹취록 공개', '경찰, 박원순 사망 관련 임순영 젠더특보 주중 소환 예정', '\"파주 아파트 수돗물서도 유충 발견\" 신고', '이인영 \"대북전단 반드시 중단…인도물자 물물교환 방식 병행\"', \"'노영민 부동산 브리핑 정정' 강민석, 사표냈다 반려\"]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    div = soup.find('div', class_ = 'list_body newsflash_body')\n",
    "    for i in div.find_all('a'):\n",
    "        result.append(i.get_text())\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    url = \"https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&sid1=001&sid2=140&oid=001&isYeonhapFlash=Y\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(crawling(soup))\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. bugs 실시간 음원차트 순위 추출하기\n",
    " \n",
    "**Exercise:** 높은 순위부터 차례대로 곡명을 출력하기\n",
    "![bugs](../image/result_bugs.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다시 여기 바닷가', '마리아 (Maria)', 'How You Like That', '여름 안에서 (Covered By 싹쓰리) (Feat. 황광희)', 'Summer Hate (Feat. 비)', '보라빛 밤 (pporappippam)', '나도 모르는 노래 (Hallelujah)', 'OHIO', 'PLAY (Feat. 창모)', 'Monster', '아직 너의 시간에 살아', 'Downtown Baby', '바다', 'Dolphin', '에잇(Prod.&Feat. SUGA of BTS)', 'Into the I-LAND', '살짝 설렜어 (Nonstop)', 'MORE & MORE', 'Apple', 'Error (Feat. Loopy)', '아로하', 'Blueming', 'Dance Monkey', '사랑하게 될 줄 알았어', '어떻게 지내 (Prod. By VAN.C)', 'Psycho', 'METEOR', '흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야', '숨 (Breath)', 'Memories', \"Don't Start Now\", '환상동화 (Secret Story of the Swan)', \"You're Cold (더 많이 사랑한 쪽이 아프대)\", 'Juice', '좋은 사람 있으면 소개시켜줘', '어떻게 이별까지 사랑하겠어, 널 사랑하는 거지', 'LALALILALA', '아무노래', '12:45 (Stripped)', '언제라도 어디에서라도', 'Maniac', '첫 줄', '너에게 난, 나에게 넌', '화려하지 않은 고백', 'Left & Right', 'WANNABE', 'In Silence', 'AWay', '나비와 고양이 (feat. 백현 (BAEKHYUN))', 'My Tale', '거짓말이라도 해서 널 보고싶어', \"i'M THE TREND\", '시작', '2002', '집돌이', 'Zombie', 'Square (2017)', '오늘따라 비가 와서 그런가 봐', 'BUNGEE (Fall in Love)', 'Say So', '작사가', 'FIESTA', 'Painkiller', '내 눈물 모아', '어떻게 지내 (답가)', '마음을 드려요', 'Love poem', '이별까지는 생각 못 했어', '안녕', 'Stuck with U', 'Birthday', '덤더럼(Dumhdurum)', '모든 날, 모든 순간 (Every day, Every Moment)', '품', '썸 타긴 뭘 타', '늦은 밤 너의 집 앞 골목길에서', '밤이 깊었네 (Drama Ver.)', '사계 (Four Seasons)', '작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)', '밤편지', 'Paris In The Rain', '그대 고운 내사랑', '불 좀 꺼줄래', '깡 Official Remix', '이제 나만 믿어요', '처음처럼', 'bad guy', '오늘도 빛나는 너에게 (To You My Light) (Feat.이라온)', '그때 그 아인', '돌덩이', 'Happy', 'Dishes', 'HIP', 'Lonely Night', 'To Die For', 'Stay Tonight', '주저하는 연인들을 위해', 'Blinding Lights', '찐이야', '사랑은 지날수록 더욱 선명하게 남아']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    table = soup.find('table', class_ = 'list trackList byChart')\n",
    "    \n",
    "    for t in table.find_all('p', class_='title'):\n",
    "        result.append(t.get_text().strip('\\n'))\n",
    "\n",
    "        \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    url = 'https://music.bugs.co.kr/chart'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(crawling(soup))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 영화 후기 수집하기\n",
    "\n",
    "**Exercise:** 네이버 영화 페이지에 있는 영화평의 제목을 수집하여 출력하기\n",
    "![moviereview](../image/result_moviereview.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['평점 알바들 정말 부끄럽게 생각해라', '영웅과 악당, 그 종이 한장 차이의 진실', '사상 최악의 영화 (스포무)', \"'다크 나이트' 보고 솔직하게 얘기해 볼께\", '히스레져의 죽음으로 조커의 연기는 파편화 되었고 영화평가는 과장됐다.', '누구도 상상못한 엄청난 반전이 있다.', '다크 나이트 - 미니어쳐 촬영 현장의 모습 ', '[다크나이트] 반성문 : 크리스토퍼 놀란 교수님께', '영화평가는 결코 과장되지 않았다[Dark Knight]', '[re]평점 알바들 정말 부끄럽게 생각해라']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ul = soup.find('ul', class_='rvw_list_area')\n",
    "    \n",
    "    for i in ul.find_all('strong'):\n",
    "        result.append(i.get_text())\n",
    "\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    url = 'https://movie.naver.com/movie/bi/mi/review.nhn?code=62586'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(crawling(soup))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 커뮤니티 댓글 수집하기\n",
    "\n",
    "네이트의 댓글을 수집하여 출력하기\n",
    "![community](../image/result_community.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['쿤맘', '우리집도 말티 \\n댕댕이들은 사랑입니당!', '겨울맞이', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 다이쁨 완전 얼짱이넹', 'leo', '우리 딸랑이', 'ㅇㅇ', '진짜 이쁘네여 ㅎㅎㅎㅎㅎ', 'ㅇㅇ', '우리집도 말티즈ㅠㅠㅠ', 'ㄱㄴ', '우리친구 자기주장이 무척 강하게 생겼네요', '세령', '말티는 사랑입니당^^', 'ㅇㄴㄻ', '오우 ses 바다 가 생각나는 헤어사타일이군요', 'ㅎ', '너무 귀엽당^^', 'ㅇㅇ', '밑에서 두번째 사진 털때문인지 뾰루퉁한거 넘 귀여워요 ㅋㅋㅋㅋㅋ!\\n울집강아지도 털때문에 가끔 눈이 화난눈 되거든요 ㅎㅎㅎㅎ 귀엽 ㅠ.ㅠ*', 'ㅇㅇ', '우리집도', 'ㅎㅎ', '달릴 때 졸귘ㅋㅋ 눈이랑 코랑 동글동글 너무 귀엽 ㅠㅠㅠㅠㅠ', '김도리', '안녕 칭구', 'ㅇㅇ', '개똥냄새 쩔게 생겼네', '소그미', '설탕 아니고 소금이', 'najing', '안녕 난 두부야', 'ㅇㅇ', '아 너모 이쁘다. 항상 건강하자 아가', '공주야사랑해', '힐링하고 갑니다❤️', '조몽실', '안뇽 ?', 'ㅇㅇ', '네츄럴 부스스.. 예쁘네요..^^']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    div = soup.find('div', id='commentDiv')\n",
    "\n",
    "    \n",
    "    for i in div.find_all('span'):\n",
    "        result.append(i.get_text().strip('\\n\\t')) # replace 2번 써줘도 됨\n",
    "\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    url = 'https://pann.nate.com/talk/350939697'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(crawling(soup))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
