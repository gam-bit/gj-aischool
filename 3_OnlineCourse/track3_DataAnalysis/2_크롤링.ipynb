{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 여러 페이지의 기사 제목 수집하기\n",
    "동아스포츠의 연예부 기사의 제목을 크롤링(1page부터 5page까지 크롤링)\n",
    "- 페이지는 쿼리부분에 있음\n",
    "- requests.get의 params 매개변수로 쿼리 변수를 추가할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['찬희 “‘한 번 다녀왔습니다’ 짧은 시간이었지만 행복♥” [종영소감]', '‘강철비2 : 정상회담’ 정우성 vs 유연석 vs 앵거슨 맥페이든 스타일 비교', '[종합] ‘밥먹다’ 김현정 “담석 괴사 위기→앨범 실패로 억대 빚더미”', '‘몸의 대화’ 브라이언,  치매 유전자 검사 결과에 충격 “약간의 쇼크 상태”', '제24회 BIFF 관객상 ‘69세’ 8월 26일 개봉…우리가 지나친 이야기', '오마이걸 아린×투모로우바이투게더 수빈, ‘뮤직뱅크’ 새 MC [공식]', '[DA:할리우드] 카니예웨스트 유세, 美대통령 선거 출마 확인', '‘마이웨이’ 설운도 “임영웅 ‘보라빛 엽서’ 부른 것 보고 히트 예감”', '‘다만 악’ 실제 타격을 그대로 담은 리얼 액션…‘스톱모션’ 기법 구현', '[DA:클립] ‘모범형사’ 장승조·이엘리야 앙숙 케미 끝? 공조 스틸컷 공개', '여자친구 새 앨범, 첫 주 6만7천장 판매…자체 최고치 [공식]', '여자친구 “어떤 콘셉트도 완벽하게 소화하고파” [화보]', '[DA:차트] 엑소 세훈&찬열 ‘10억뷰’, 가온 주간 앨범 1위', '[DAY컷] 이정현, 깜찍한 ‘바비인형’ 변신 “‘집사부일체’ 멤버 고생했어요…', '보아 “데뷔 20주년, 이제 팬들이 친구 같아” [화보]', '‘밥먹다’ 김현정 “2015년 자작곡 망해 빚만 수억원”', '리사, 21일 ‘Lazy Love’ 발매', '‘밥먹다’ 크리스티나 “母 폐암 4기…코로나 사태로 친정 못 가”', '‘밥먹다’ 김현정 “담석 괴사 돼 죽을 고비→덤프트럭과 교통사고”', '‘밥먹다’ 김현정 “담석 괴사 돼 죽을 고비→덤프트럭과 교통사고”', '‘반도’ 이정현, 23일 관객들과의 만남 참석…연상호 감독x이동진 평론가와 함…', '‘트레인’ 경수진, B세계 소시오패스 형사 1인 2역 열연', '[DAY컷] 유아인, 취중 어리광 폭발…식당 아주머니 품속에 폭', '김나영, 오늘(20일) ‘그놈이 그놈이다’ 첫 OST 발표', '‘안 싸우면 다행이야’ 첫 방송…안정환×이영표 촬영 10분 만에 포기?', '‘오케이 마담’ 8월 12일 개봉 확정…뜻밖의 반전 매력 포스터', '‘지숙♥’ 이두희, 해킹 피해 경고 “절대 클릭 금지!”', '이진혁, 신개념 ‘만.통.회’ 개최…부캐 잇는 멀티캐 도전', '[DAY컷] “꼬마 나래, 작구나”…박나래, 어린 시절 공개', '‘컴백’ 전소미, 매혹적 레드 앨범 커버 공개', '‘킹스맨 : 퍼스트 에이전트’ 1차 메인 포스터·예고편 공개', '‘출사표’ 나나 “연기 호평 감사, 망가짐에 걱정無” [일문일답]', '‘악의 꽃’ 이준기 “백희성, 따뜻함 뒤 서늘함 감춘 인물”', '[DAY컷] ‘브람스를 좋아하세요’ 박은빈, 긴 생머리 청순 음대생 변신', '레드벨벳 아이린&슬기, 후속곡 ‘놀이’ 오늘 공개', '‘국제수사’ 1차 예고편 공개…곽도원, 짠내나는 글로벌 수사', '‘예리한방’ 아이린&슬기 출연…막내 예리 지원사격', 'YG 신인 트레저, 8월7일 데뷔 [공식]', '청하, ‘청하나라의 별하랑’ 온라인 팬미팅 마무리', '‘바람피면 죽는다’ 조여정X고준 출연 확정, 10월 첫방송 [공식]', '‘가족입니다’ 한예리→신재하, 위로와 공감 선사했던 가족의 마지막 이야기는?', '[DA:클립] ‘개는 훌륭하다’ 강형욱, 이경규에 감탄…안락사 위기견 행동교정', '빅톤 한승우 “‘승우야 네 인생을 응원해’ 말 듣고 오열” [화보]', '디원스, 발라드로 증명한 찰떡돌…‘너를 그린다’ 활동 종료', '‘죽지 않는 인간들의 밤’ 9월 개봉…신정원표 코믹 스릴러 컴백', '‘최애엔터인먼트’ 후이가 왜 거기서 나와…트롯 야망돌 한 발짝', '[DA:클립] ‘그놈이 그놈이다’ 황정음·서지훈, 고백에 서먹…방황하는 눈동자', '‘사이코지만 괜찮아’ 박규영, 귀여움+애틋함 지닌 하이브리드 술 주정', '‘너의 시선이 머무는 곳에’ 장의수 “사랑에 경계 없다” [화보]', '‘거짓말의 거짓말’ 이유리 “지은수 아픔에 공감”…깊은 모성 연기 예고', '유연석·이광수, 킹콩 재계약…10년 의리 이어간다 [공식]', '[TV북마크] ‘서울촌놈’ 이시언, 남포동서 인기 실감…찐 셀럽 인증 (종합)', '[TV북마크] ‘슈돌’ 하영, 폴더 인사→돈 계산 완벽…세젤귀 영업왕 (종합)', '‘아이콘택트’ 강호동, 부캐 희망 사항 고백 “아이돌로 살아보고파”', '‘드루와’ 천명훈 “NRG 멤버→트로트 전향”…영탁과 맞대결 심경 고백', '[TV북마크] ‘복면가왕’ 장미여사 3연승 성공, 지화자=김선경 ‘최고 11.…', '현아♥던 화보, 범접불가 커플 아우라 [화보]', '[DA:투데이] ‘퀴즈 위의 아이돌’ 오늘(20일) 첫방송, 新가족오락관 예고…', '[TV북마크] ‘1박2일’ 이장희 출연→독도 입성, 최고 18.9% ‘동시간 …', '신지·이채윤, 오늘(20일) ‘우리말 겨루기’ 출연', '홍은기, 8월2일 컴백…성장 담은 ‘Flower’ 발표', '‘컴백’ 디코이, 업그레이드 감성 ‘GO AWAY’ 티저 공개', '[TV북마크] ‘당나귀귀’ 송훈 셰프 첫등장→도티 집 ‘최고 9.7%’ (종합…', '[TV북마크] ‘선녀들’ 설민석 “전국민이 독도 가슴에 담아야”…최고 6% (…', '[TV북마크] ‘미우새’ 하하 “♥별, 잘 때 예뻐”…탁궁짠 절교 위기 ‘19…', '[TV북마크] ‘집사부일체’ 이정현표 탑골 콘서트 성료…이승기 고음 화제 (종…', '[TV북마크] ‘사이코지만 괜찮아’ 김수현, ♥서예지와 억지 이별→그리움 (종…', '[TV북마크] ‘구해줘! 홈즈’ 오나라·박지현 덕팀 승리, 최고 8.8% (종…', '[TV북마크] ‘유랑마켓’ 김종민, 맥시멀리스트+그루밍족 집털이 (종합)', '트와이스 8월9일 첫 온라인 콘서트 개최 [공식]', '[TV북마크] ‘비긴어게인 코리아’ 클래식과 환상 콜라보 (종합)', '서은광, 온라인 콘서트 성료…비투비 특급 우정', '[TV북마크] ‘뭉쳐야 찬다’ 양준혁 결혼 예고→김요한 첫 해트트릭 (종합)', '‘컴백’ 에이프릴 채경·채원, 청량 여름 콘셉트 포토 공개', '[인터뷰] 서지혜, 차도녀→러블리…“이런 수식어는 처음이에요”', '[TV북마크] ‘바람과 구름과 비’ 전광렬, 박시후 전략 간파…전세역전 (종합…', '[TV북마크] ‘한번다녀왔습니다’ 천호진·이정은, 남매 상봉 불발…자체 최고 …', '[DA:박스] ‘반도’ 5일만에 180만 돌파…극장가 활력↑', '우즈(조승연), 실력파 솔로 존재감…‘EQUAL’ 활동 성료', '[TV북마크] ‘슈돌’ 도플갱어 가족, 선한 영향력…캥거루 집업 판매 기부 (…', '골든차일드, 자아 찾기 3부작 피날레 ‘ONE’ 활동 마무리', '[DA:클립] ‘그놈이 그놈이다’ 윤현민, 황정음 향한 직진♥ “사귀시죠”', '싸이→현아, 피네이션 기습 티저…30일 컴백 주인공은?', '‘무엇이든 물어보살’ 김정민→조이현(조승희), 끼쟁이 의뢰인들 출동', '[DA:차트] 방탄소년단, 日 오리콘 앨범 5일 연속 1위', '[원픽! 위클리 핫이슈] 남진 데뷔 55주년 기념 헌정식…후배가수들, 축하무대…', '[연예뉴스 HOT④] 한소희, 모친 채무 논란으로 구설수', '[연예뉴스 HOT③] SNS로 해킹 메시지 받은 홍진영 “휴”', '[연예뉴스 HOT②] tvN ‘여름방학’, 日 게임 표절·왜색 논란', '[연예뉴스 HOT①] 개그맨 노우진, 음주운전 혐의로 경찰 입건', '듀스·룰라·유피 소환…요즘은 ‘복고풍 음악’이 대세', '[뉴스인사이드] SNS·유튜브 광고의 세계…“구독자 수십만 땐 최대 5000만…', '저 소녀 누구야…영화 ‘반도’ 흥행 순항 속 10대 배우 이레 연기 주목', '[종합] 양준혁 결혼, 여자친구 누구? ‘뭉찬’서 예비신부 공개', '[DA:리뷰] ‘미우새’ 하하가 밝힌 #아내 별 #정관수술 #김종국 결혼 (종…', '‘미우새’ 하하 “아내 별 제일 무서울 때? 술 깨고 난 후”', '‘미우새’ 하하 정관수술 고백 “별과 셋째로 끝, 묶었다”']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    \n",
    "    result = []\n",
    "    ul = soup.find('ul', class_=\"list_news\")\n",
    "    \n",
    "    for i in ul.find_all('span', class_='tit'):\n",
    "        result.append(i.get_text())\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def main() :\n",
    "    answer = []\n",
    "    url = \"https://sports.donga.com/ent\"\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        req = requests.get(url, params={'p': i*20 + 1})\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        \n",
    "        answer += crawling(soup)\n",
    "\n",
    "    \n",
    "    print(answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 각 기사의 href 수집\n",
    "동아스포츠의 연예부 기사의 href를 수집하기\n",
    "![href](../image/result_href.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://sports.donga.com/ent/article/all/20200720/102067160/2', 'https://sports.donga.com/ent/article/all/20200720/102067160/2', 'https://sports.donga.com/ent/article/all/20200720/102067160/2', 'https://sports.donga.com/ent/article/all/20200720/102067061/1', 'https://sports.donga.com/ent/article/all/20200720/102067061/1', 'https://sports.donga.com/ent/article/all/20200720/102067061/1', 'https://sports.donga.com/ent/article/all/20200720/102067042/2', 'https://sports.donga.com/ent/article/all/20200720/102067042/2', 'https://sports.donga.com/ent/article/all/20200720/102067042/2', 'https://sports.donga.com/ent/article/all/20200720/102066776/1', 'https://sports.donga.com/ent/article/all/20200720/102066776/1', 'https://sports.donga.com/ent/article/all/20200720/102066776/1', 'https://sports.donga.com/ent/article/all/20200720/102066376/1', 'https://sports.donga.com/ent/article/all/20200720/102066376/1', 'https://sports.donga.com/ent/article/all/20200720/102066376/1', 'https://sports.donga.com/ent/article/all/20200720/102065934/2', 'https://sports.donga.com/ent/article/all/20200720/102065934/2', 'https://sports.donga.com/ent/article/all/20200720/102065934/2', 'https://sports.donga.com/ent/article/all/20200720/102065842/2', 'https://sports.donga.com/ent/article/all/20200720/102065842/2', 'https://sports.donga.com/ent/article/all/20200720/102065842/2', 'https://sports.donga.com/ent/article/all/20200720/102065875/1', 'https://sports.donga.com/ent/article/all/20200720/102065875/1', 'https://sports.donga.com/ent/article/all/20200720/102065875/1', 'https://sports.donga.com/ent/article/all/20200720/102065721/2', 'https://sports.donga.com/ent/article/all/20200720/102065721/2', 'https://sports.donga.com/ent/article/all/20200720/102065721/2', 'https://sports.donga.com/ent/article/all/20200720/102065613/2', 'https://sports.donga.com/ent/article/all/20200720/102065613/2', 'https://sports.donga.com/ent/article/all/20200720/102065613/2', 'https://sports.donga.com/ent/article/all/20200720/102065610/1', 'https://sports.donga.com/ent/article/all/20200720/102065610/1', 'https://sports.donga.com/ent/article/all/20200720/102065610/1', 'https://sports.donga.com/ent/article/all/20200720/102065538/1', 'https://sports.donga.com/ent/article/all/20200720/102065538/1', 'https://sports.donga.com/ent/article/all/20200720/102065538/1', 'https://sports.donga.com/ent/article/all/20200720/102065474/1', 'https://sports.donga.com/ent/article/all/20200720/102065474/1', 'https://sports.donga.com/ent/article/all/20200720/102065474/1', 'https://sports.donga.com/ent/article/all/20200720/102065429/2', 'https://sports.donga.com/ent/article/all/20200720/102065429/2', 'https://sports.donga.com/ent/article/all/20200720/102065429/2', 'https://sports.donga.com/ent/article/all/20200720/102065329/1', 'https://sports.donga.com/ent/article/all/20200720/102065329/1', 'https://sports.donga.com/ent/article/all/20200720/102065329/1', 'https://sports.donga.com/ent/article/all/20200720/102065190/1', 'https://sports.donga.com/ent/article/all/20200720/102065190/1', 'https://sports.donga.com/ent/article/all/20200720/102065190/1', 'https://sports.donga.com/ent/article/all/20200720/102065180/1', 'https://sports.donga.com/ent/article/all/20200720/102065180/1', 'https://sports.donga.com/ent/article/all/20200720/102065180/1', 'https://sports.donga.com/ent/article/all/20200720/102065144/1', 'https://sports.donga.com/ent/article/all/20200720/102065144/1', 'https://sports.donga.com/ent/article/all/20200720/102065144/1', 'https://sports.donga.com/ent/article/all/20200720/102065124/2', 'https://sports.donga.com/ent/article/all/20200720/102065124/2', 'https://sports.donga.com/ent/article/all/20200720/102065124/2']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup) :\n",
    "    # soup에 저장되어 있는 각 기사에 접근할 수 있는 href들을 담고 있는 리스트를 반환해주세요.\n",
    "    result = []\n",
    "    \n",
    "    ul = soup.find('ul', class_='list_news')\n",
    "    \n",
    "    for i in ul.find_all('a'):\n",
    "        result.append(i.attrs['href'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    list_href = []\n",
    "\n",
    "    url = \"https://sports.donga.com/ent?p=1&c=02\"\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, \"html.parser\")\n",
    "\n",
    "    print(get_href(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 각 기사의 href 수집\n",
    "네이트의 최신뉴스 목록에서 각 기사의 href를 수집하기\n",
    "![href](../image/result_natehref.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://news.nate.com/view/20200720n30805?mid=n0100', 'https://news.nate.com/view/20200720n30804?mid=n0100', 'https://news.nate.com/view/20200720n30803?mid=n0100', 'https://news.nate.com/view/20200720n30802?mid=n0100', 'https://news.nate.com/view/20200720n30801?mid=n0100', 'https://news.nate.com/view/20200720n30800?mid=n0100', 'https://news.nate.com/view/20200720n30799?mid=n0100', 'https://news.nate.com/view/20200720n30798?mid=n0100', 'https://news.nate.com/view/20200720n30797?mid=n0100', 'https://news.nate.com/view/20200720n30796?mid=n0100', 'https://news.nate.com/view/20200720n30795?mid=n0100', 'https://news.nate.com/view/20200720n30794?mid=n0100', 'https://news.nate.com/view/20200720n30793?mid=n0100', 'https://news.nate.com/view/20200720n30792?mid=n0100', 'https://news.nate.com/view/20200720n30791?mid=n0100', 'https://news.nate.com/view/20200720n05737?mid=n0100', 'https://news.nate.com/view/20200720n30788?mid=n0100', 'https://news.nate.com/view/20200720n30787?mid=n0100', 'https://news.nate.com/view/20200720n30401?mid=n0100', 'https://news.nate.com/view/20200720n29930?mid=n0100']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup) :\n",
    "    \n",
    "    result = []\n",
    "    div = soup.find('div', class_='postSubjectContent')\n",
    "    \n",
    "    for i in div.find_all('a'):\n",
    "        result.append('https:' + i.attrs['href'])\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    url = \"https://news.nate.com/recent?mid=n0100\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    print(get_href(soup))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★4. sbs 뉴스 최신 기사 목록의 내용 수집하기\n",
    " sbs 뉴스의 최신 기사 목록의 href를 추출하고, href로 접근할 수 있는 기사들의 내용을 추출하기\n",
    " \n",
    " - get_href 함수 : 각 기사의 href 주소 \n",
    " - crawling 함수 : 기사 내용 \n",
    "\n",
    "![sbs](../image/result_sbs.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup) :\n",
    "    \n",
    "    result = []\n",
    "    div = soup.find('div', class_ = 'w_news_list type_issue')\n",
    "    \n",
    "    for i in div.find_all('a', class_='news'):\n",
    "                # ★추출한 하이퍼링크 외에 그 앞에 무엇이 붙는지 확인할 것★\n",
    "        result.append(\"https://news.sbs.co.kr/\" + i.attrs['href'])\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def crawling(soup):\n",
    "    \n",
    "    div = soup.find('div', class_ = 'text_area')\n",
    "    \n",
    "    return div.get_text()\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    url = \"https://news.sbs.co.kr/news/newsflash.do?plink=GNB&cooper=SBSNEWS\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    news_url = get_href(soup) # 뉴스 urls\n",
    "    \n",
    "    news_content = []\n",
    "    for news in news_url:\n",
    "        req_news = requests.get(news)\n",
    "        soup_news = BeautifulSoup(req_news.text, 'html.parser')\n",
    "        result = crawling(soup_news)\n",
    "        news_content.append(result)\n",
    "    \n",
    "    return news_content\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★5. 다양한 섹션의 속보 기사 href 추출하기\n",
    "네이버 뉴스 속보 페이지에는 여러 페이지가 있음. url의 쿼리 부분에서 sid1의 값에 따라 섹션이 결정된다. input 함수로 섹션을 입력.\n",
    "\n",
    "- get_request 함수 : 쿼리와 get요청을 담고, requests 객체를 반환\n",
    "- get_href 함수 : 섹션별로 나뉘어진 목록에 있는 기사들의 href를 추출\n",
    "\n",
    "![href](../image/result_sectionhref.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\n",
      "  > 정치\n",
      "['https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=031&aid=0000548674', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=031&aid=0000548674', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=022&aid=0003486482', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=022&aid=0003486482', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=056&aid=0010872404', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=056&aid=0010872404', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=421&aid=0004768247', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=421&aid=0004768247', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=469&aid=0000517956', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=469&aid=0000517956', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=031&aid=0000548672', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=031&aid=0000548672', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760272', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760272', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760271', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760271', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760270', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760270', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760269', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=001&aid=0011760269']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup):\n",
    "    \n",
    "    result = []\n",
    "    ul = soup.find('ul', class_='type06_headline')\n",
    "    for a in ul.find_all('a'):\n",
    "        result.append(a['href'])\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_request(section):    \n",
    "    url = \"https://news.naver.com/main/list.nhn\"\n",
    "    \n",
    "    \n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    ul = soup.find('ul', class_='nav')\n",
    "    for a in ul.find_all('a'):\n",
    "        if section in a.get_text():\n",
    "            s_url = \"https://news.naver.com/\" + a['href']\n",
    "            return requests.get(s_url)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    list_href = get_href(soup)\n",
    "    print(list_href)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\n",
      "  > 정치\n",
      "['https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=005&aid=0001344113', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=005&aid=0001344113', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=437&aid=0000243233', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=437&aid=0000243233', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=310&aid=0000079682', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=310&aid=0000079682', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=088&aid=0000656731', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=088&aid=0000656731', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=018&aid=0004695345', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=018&aid=0004695345', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=025&aid=0003019502', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=025&aid=0003019502', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=011&aid=0003771903', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=011&aid=0003771903', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=215&aid=0000887569', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=215&aid=0000887569', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=079&aid=0003386305', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=079&aid=0003386305', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=015&aid=0004384666', 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=100&oid=015&aid=0004384666']\n"
     ]
    }
   ],
   "source": [
    "# 답안 --> 쿼리를 이용(requests.get)\n",
    "#      --> get_request함수가 다르게 생겼음\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_href(soup):\n",
    "    \n",
    "    result = []\n",
    "    ul = soup.find('ul', class_='type06_headline')\n",
    "    for a in ul.find_all('a'):\n",
    "        result.append(a['href'])\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_request(section):    \n",
    "    \n",
    "    url = \"https://news.naver.com/main/list.nhn\"\n",
    "    \n",
    "    sections = {\n",
    "        \"정치\" : 100,\n",
    "        \"경제\" : 101,\n",
    "        \"사회\" : 102,\n",
    "        \"생활\" : 103,\n",
    "        \"세계\" : 104,\n",
    "        \"과학\" : 105\n",
    "    }\n",
    "    # https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=101\n",
    "    req = requests.get(url, params={'sid1': sections[section]})\n",
    "\n",
    "    return req\n",
    "\n",
    "def main():\n",
    "    \n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    list_href = get_href(soup)\n",
    "    print(list_href)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★6. 다양한 섹션의 속보 기사 내용 추출하기\n",
    "기사의 href에서 내용을 추출하기. \n",
    "\n",
    "![content](../image/result_sectioncontent.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\n",
      "  > 정치\n",
      "[\"(서울=뉴스1) 구윤성 기자 = 21일 서울 미군 용산기지 내 장교숙소부지에서 열린 '함께 그리는 용산공원 부분개방 행사'에서 정세균 국무총리를 비롯한 참석자들이 기념비 제막 기념촬영을 하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지\", \"(서울=뉴스1) 구윤성 기자 = 21일 서울 미군 용산기지 내 장교숙소부지에서 열린 '함께 그리는 용산공원 부분개방 행사'에서 정세균 국무총리를 비롯한 참석자들이 기념비 제막 기념촬영을 하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지\", \"(서울=뉴스1) 구윤성 기자 = 21일 서울 미군 용산기지 내 장교숙소부지에서 열린 '함께 그리는 용산공원 부분개방 행사'에서 정세균 국무총리를 비롯한 참석자들이 기념비 제막 기념촬영을 하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지\", \"(서울=뉴스1) 구윤성 기자 = 21일 서울 미군 용산기지 내 장교숙소부지에서 열린 '함께 그리는 용산공원 부분개방 행사'에서 정세균 국무총리를 비롯한 참석자들이 기념비 제막 기념촬영을 하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지\", '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 정 총리는 \"용산기지를 국민께 돌려드리는 역사적인 첫걸음에 함께하게 돼 매우 기쁘다\"며 \"용산공원을 국민과 함께하는 진정한 휴식공간으로 만들어가겠다\"고 밝혔다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 정 총리는 \"용산기지를 국민께 돌려드리는 역사적인 첫걸음에 함께하게 돼 매우 기쁘다\"며 \"용산공원을 국민과 함께하는 진정한 휴식공간으로 만들어가겠다\"고 밝혔다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다.  2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다.  2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리와 김현미 국토교통부 장관, 유홍준 용산공원추진위원장을 비롯한 참석자들이 21일 오후 서울 용산공원에서 열린 ‘함께 그리는 용산공원 부분개방 행사’에서 전시물을 관람하며 이동하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지', '(서울=뉴스1) 구윤성 기자 = 정세균 국무총리가 21일 오후 서울 용산구 용산공원 부지 내에서 열린 제2회 용산공원 추진위원회 회의를 주재하고 있다. 2020.7.21/뉴스1photo@news1.kr▶ 네이버 메인에서 [뉴스1] 구독하기!▶  뉴스1 바로가기 ▶  코로나19 뉴스 © 뉴스1코리아(news1.kr), 무단 전재 및 재배포 금지']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def crawling(soup):\n",
    "    div = soup.find('div', class_= '_article_body_contents')\n",
    "    \n",
    "    result = div.get_text().replace('\\n', '').replace('\\t', '').replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", '')\n",
    "    \n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_href(soup):\n",
    "    \n",
    "    result = []\n",
    "    ul = soup.find('ul', class_='type06_headline')\n",
    "    for a in ul.find_all('a'):\n",
    "        result.append(a['href'])\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_request(section):    \n",
    "    url = \"https://news.naver.com/main/list.nhn\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    ul = soup.find('ul', class_='nav')\n",
    "    for a in ul.find_all('a'):\n",
    "        if section in a.get_text():\n",
    "            s_url = \"https://news.naver.com/\" + a['href']\n",
    "            return requests.get(s_url)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    section = input('\"정치\", \"경제\", \"사회\", \"생활\", \"세계\", \"과학\" 중 하나를 입력하세요.\\n  > ')\n",
    "    req = get_request(section)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    list_href = get_href(soup)\n",
    "    \n",
    "    result = []\n",
    "    for href in list_href:\n",
    "        req_href = requests.get(href)\n",
    "        soup = BeautifulSoup(req_href.text, 'html.parser')\n",
    "        result.append(crawling(soup))\n",
    "    print(result)\n",
    "        \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 특정 영화 리뷰 추출하기\n",
    "네이버 영화 페이지의 영화 리뷰 제목을 크롤링하기. 영화 제목이 입력으로 주어지로, 해당 영화 리뷰 결과 보여주기.\n",
    "\n",
    "- get_url : 영화 제목 입력받고 제목을 검색했을 때 나오는 url을 반환(문자열 결합 사용)\n",
    "- get_href : 검색 결과, 가장 위에 있는 영화의 href를 반환\n",
    "- crawling : 영화의 href로 접근하여, 해당 영화의 리뷰 목록을 크롤링\n",
    "\n",
    "![review](../image/result_moviereview2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화 제목을 입력하세요. \n",
      " > 기생충\n",
      "['ㅋㅋ 이미 영화가 다보이네', '(스포, 해석)인간은 높은 곳 낮은 곳 어디에나 존재하고, 존재하지 않는다.', '봉준호가 계층 계급에 집착하는 이유', '자본주의비판노노..인식에대하여 +한국판 웨스앤더슨의 대칭의 미학', '<결말해석> 누가 진짜 기생충이게? 원주민 행세를 하는 가짜 인디언을 찾아야 한다.', \"기생충, 사회 시스템과 '공생' 에 관한 사회 스릴러!\", '지렷다....', '기생충 후기, 결말은 빈부격차 현실을 잘 표현한 영화다..', '진짜 모르겠다.', '대충 어떤식의 영화일지 감이 옴']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup):\n",
    "    ul = soup.find('ul', class_='rvw_list_area')\n",
    "    result = []\n",
    "    for title in ul.find_all('strong'):\n",
    "        result.append(title.get_text())\n",
    "    return result\n",
    "\n",
    "def get_href(soup):\n",
    "    \n",
    "    div = soup.find('div', class_='info_main')\n",
    "    a = div.find('a')\n",
    "    href = a['href'].replace(\"basic\", \"review\")\n",
    "    return href\n",
    "    \n",
    "\n",
    "\n",
    "def get_url(movie):\n",
    "    url = \"https://search.naver.com/search.naver?query=\" + movie\n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    movie = input('영화 제목을 입력하세요. \\n > ')\n",
    "    \n",
    "    url = get_url(movie)\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    movie_url = get_href(soup)\n",
    "#     print(movie_url)\n",
    "    movie_req = requests.get(movie_url)\n",
    "    movie_soup = BeautifulSoup(movie_req.text, 'html.parser')\n",
    "    print(crawling(movie_soup))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
